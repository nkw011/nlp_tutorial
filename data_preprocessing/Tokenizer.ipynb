{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK, spaCy, torchtext를 이용하여 영어 토큰화(English Tokenization)작업 수행하기\n",
        "\n",
        "NLP Task를 수행하기 위해 필요한 데이터 전처리 과정 중 Tokenization(토큰화)를 직접 실습해본다.\n",
        "\n",
        "Python 기본 라이브러리를 이용해 토큰화를 진행할 수도 있지만 시간이 오래걸리고 고려해야할 점이 많다.\n",
        "* 구두점, 특수문자를 단순하게 제외해서는 안되는 경우: Ph.D.(학위)\n",
        "* 줄임말\n",
        "* 한 단어인데 띄어쓰기가 안에 있는 경우: 상표 등\n",
        "* 공백 단위의 토큰화를 적용할 수 없는 경우: 's (소유격), don't, doesn't (do + not 형태) 등\n",
        "\n",
        "\n",
        "공개되어있는 자연어 처리 Library를 사용하여 빠르게 토큰화를 하는 방법을 알아본다.\n",
        "(sub-word 단위의 토큰화는 여기서 수행하지 않는다.)\n",
        "\n",
        "※ 실행환경: colab"
      ],
      "metadata": {
        "id": "Liwl5VcGzONr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. NLTK\n",
        "\n",
        "NLTK는 Natural Language Toolkik의 약자로 교육용으로 개발된 자연어 처리 및 문서 분석용 Python Package이다.\n",
        "\n",
        "주요 기능\n",
        "* 토큰화(Tokenization)\n",
        "* 말뭉치(Corpus)\n",
        "* 형태소 분석, 품사 태깅(PoS) 등\n",
        "\n",
        "NLTK는 pip를 이용해 설치할 수 있다."
      ],
      "metadata": {
        "id": "oxgB01SL0kCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "6G-q4SQr7_Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "완료가 되면 다음과 같이 nltk가 설치되어 있는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "MGei4qfT8GxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL9T-DvvyuV3"
      },
      "outputs": [],
      "source": [
        "!pip show nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK의 Tokenizer(토크나이저)를 사용하기 위해서는 데이터(NLTK Data)를 설치해야한다.\n",
        "\n",
        "nltk를 import하고 nltk.download()를 이용해서 토큰화에 필요한 데이터를 설치할 수 있다.\n",
        "* nltk.download(): GUI로 이루어진 NLTK 다운로더가 나타난다. 필요한 데이터를 클릭해 설치할 수 있다.\n",
        "* nltk.download('data_name'): download()의 인자로 필요한 데이터의 이름을 넘겨주면 해당 데이터만 다운로드 받을 수 있다.\n",
        "    * nltk.download('popular')"
      ],
      "metadata": {
        "id": "o-8fS1Ku8PU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "TJNhp_iJ6Yj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. word_tokenize()\n",
        "\n",
        "word_tokenize()를 이용해 문장을 토큰화할 수 있다."
      ],
      "metadata": {
        "id": "0S_X6Xwx-ee7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "N4cMxPNc-w4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'David\\'s book wasn\\'t famous, but his family loved his book.'"
      ],
      "metadata": {
        "id": "7cyvUB1Z-5Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnk3Exd_E5f",
        "outputId": "df60441e-1c54-4ff9-9128-48a5789551dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['David',\n",
              " \"'s\",\n",
              " 'book',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'famous',\n",
              " ',',\n",
              " 'but',\n",
              " 'his',\n",
              " 'family',\n",
              " 'loved',\n",
              " 'his',\n",
              " 'book',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. WordPunctTokenizer\n",
        "\n",
        "WordPunctTokenizer는 work_tokenize와는 달리 '(구두점)을 별도의 토큰으로 구분해서 토큰화를 진행한다."
      ],
      "metadata": {
        "id": "sdqNveJB_V7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "sbW2OoWp_lvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punct_tokenizer = WordPunctTokenizer()\n",
        "\n",
        "punct_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8q9OE7B_plJ",
        "outputId": "0ff776bd-cc88-410a-eb9e-7facf86a767e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['David',\n",
              " \"'\",\n",
              " 's',\n",
              " 'book',\n",
              " 'wasn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'famous',\n",
              " ',',\n",
              " 'but',\n",
              " 'his',\n",
              " 'family',\n",
              " 'loved',\n",
              " 'his',\n",
              " 'book',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wasn't이 wasn, ', t 로  분리된 것을 알 수 있다."
      ],
      "metadata": {
        "id": "AUxwQFJp_z8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. sent_tokenize()\n",
        "\n",
        "여러 문장으로 이루어진 text를 1개의 문장씩 토큰화하는 함수이다."
      ],
      "metadata": {
        "id": "sqWiVif2AFzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "XuUom99aATyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = 'David\\'s book wasn\\'t famous, but his family loved his book. Seventy years later, his book began to be known to the public.'"
      ],
      "metadata": {
        "id": "CJK6U2xeBvI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkpd9zIjAb2i",
        "outputId": "82e76cbe-bb1b-411c-e5a7-2fa947a5195e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"David's book wasn't famous, but his family loved his book.\",\n",
              " 'Seventy years later, his book began to be known to the public.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. 불용어(stopword) 처리하기\n",
        "\n",
        "자주 등장하지만 실제 의미 분석을 하거나 작업을 수행하는데 크게 기여하지 않는 단어들을 불용어(stopword)라고 한다.\n",
        "\n",
        "불용어는 문장의 길이를 늘리기 때문에 실제 학습할 때 불용어를 제거하는 작업을 수행할 수도 있다."
      ],
      "metadata": {
        "id": "Yj5chjn-ClYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "EoYRndoKA4Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word_list = stopwords.words('english')"
      ],
      "metadata": {
        "id": "DU85LyYQD4av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in stop_word_list[:10]:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP4t0cLfD8Pe",
        "outputId": "011d207d-2647-4ac0-84eb-bca99c17c5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i\n",
            "me\n",
            "my\n",
            "myself\n",
            "we\n",
            "our\n",
            "ours\n",
            "ourselves\n",
            "you\n",
            "you're\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_list = [ token for token in word_tokenize(text) if token not in stop_word_list]\n",
        "print(token_list)\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_hZhhYOEBtX",
        "outputId": "2e24a369-f3a5-48b0-99a5-165990588f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['David', \"'s\", 'book', \"n't\", 'famous', ',', 'family', 'loved', 'book', '.']\n",
            "['David', \"'s\", 'book', 'was', \"n't\", 'famous', ',', 'but', 'his', 'family', 'loved', 'his', 'book', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고\n",
        "* [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)\n",
        "* [NLTK 자연어 처리 패키지](https://datascienceschool.net/03%20machine%20learning/03.01.01%20NLTK%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%20%ED%8C%A8%ED%82%A4%EC%A7%80.html)"
      ],
      "metadata": {
        "id": "GjG7OzikyBWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. spaCy \n",
        "\n",
        "spaCy는 Python과 Cython으로 작성된 고급 자연어 처리를 위한 Python Package이다.\n",
        "\n",
        "주요 기능\n",
        "* POS Tagging\n",
        "* Morphology\n",
        "* Lemmaztization\n",
        "* Tokenization\n",
        "* Named Entities 등\n",
        "\n",
        "[spaCy - Tokenization](https://spacy.io/usage/linguistic-features#tokenization), [spaCy - Processing Pipelines](https://spacy.io/usage/processing-pipelines)에서 자세한 내용을 확인할 수 있다.\n",
        "\n",
        "\n",
        "spaCy는 pip를 이용하여 설치할 수 있다."
      ],
      "metadata": {
        "id": "Vm93g8HaEjfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "MgGoCeZ_EW7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설치가 완료되면 다음과 같이 확인할 수 있다."
      ],
      "metadata": {
        "id": "V08BOfR8ZrH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show spacy"
      ],
      "metadata": {
        "id": "jgCGmMCoGg09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy 역시 토큰화에 필요한 데이터를 다운로드 해야한다."
      ],
      "metadata": {
        "id": "-aq0F3N7Gu0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "RL97e3hCGiQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. spaCy를 이용해 토큰화 수행하기"
      ],
      "metadata": {
        "id": "ykTk5PqHHcLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "76UCyf1lHXBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "b-FrMWhvHjlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'David\\'s book wasn\\'t famous, but his family loved his book.'"
      ],
      "metadata": {
        "id": "DwUdzBFcHn7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in spacy_en.tokenizer(text):\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6iQ-RbYHraH",
        "outputId": "eebbd4d9-03ae-438d-e8ae-d152198ee6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "David\n",
            "'s\n",
            "book\n",
            "was\n",
            "n't\n",
            "famous\n",
            ",\n",
            "but\n",
            "his\n",
            "family\n",
            "loved\n",
            "his\n",
            "book\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy를 이용해 토큰화를 수행하면 기본적으로 토큰외에도 PoS(품사), lemma등의 정보를 알 수 있다."
      ],
      "metadata": {
        "id": "w3vB81LjH6eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in spacy_en.tokenizer(text):\n",
        "    print(f\"token: {token.text}, PoS: {token.pos_}, lemman: {token.lemma_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzIK34FKHtqT",
        "outputId": "79bdbea4-8b1b-4707-b403-d536f81f1331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: David, PoS: , lemman: \n",
            "token: 's, PoS: , lemman: \n",
            "token: book, PoS: , lemman: \n",
            "token: was, PoS: , lemman: \n",
            "token: n't, PoS: , lemman: \n",
            "token: famous, PoS: , lemman: \n",
            "token: ,, PoS: , lemman: \n",
            "token: but, PoS: , lemman: \n",
            "token: his, PoS: , lemman: \n",
            "token: family, PoS: , lemman: \n",
            "token: loved, PoS: , lemman: \n",
            "token: his, PoS: , lemman: \n",
            "token: book, PoS: , lemman: \n",
            "token: ., PoS: , lemman: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. 불용어(stopword)"
      ],
      "metadata": {
        "id": "BW7Q-6-WI5DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "metadata": {
        "id": "EDKEHv9MITSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, stop_word in enumerate(stop_words):\n",
        "    if i == 10: break\n",
        "    print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLk_2RsQJIc4",
        "outputId": "8ba17734-aee5-4fc9-8fa6-aa704d7afeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "very\n",
            "become\n",
            "twelve\n",
            "hereupon\n",
            "into\n",
            "say\n",
            "‘ll\n",
            "each\n",
            "throughout\n",
            "’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. torchtext\n",
        "\n",
        "자연어처리를 위해 만들어진 PyTorch 라이브러리이다.\n",
        "* [Docs](https://pytorch.org/text/stable/index.html)\n",
        "\n",
        "pip를 이용해 설치할 수 있다. PyTorch version 주의하여 적합한 version을 설치해야한다. \n",
        "* [Installation Guide](https://github.com/pytorch/text#installation)"
      ],
      "metadata": {
        "id": "vLaaD9iDKi38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "XoBD7WcHMjSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torchtext"
      ],
      "metadata": {
        "id": "6GFySuRjJtA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. get_tokenizer()를 이용해 토큰화 수행하기\n",
        "\n",
        "[get_tokenizer()](https://pytorch.org/text/stable/data_utils.html?highlight=get_tok#torchtext.data.utils.get_tokenizer)를 이용하여 torchtext에서 사용되는 tokenizer를 불러올 수 있다."
      ],
      "metadata": {
        "id": "fXaPpJZtNBLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "VJwwNZSRMw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'David\\'s book wasn\\'t famous, but his family loved his book.'"
      ],
      "metadata": {
        "id": "WcJ3oN49NNkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text)"
      ],
      "metadata": {
        "id": "y1oXU5kvN1Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zfO4hvFN4dy",
        "outputId": "0ad68e6a-d4ec-45f0-daa5-29eff5226c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david',\n",
              " \"'\",\n",
              " 's',\n",
              " 'book',\n",
              " 'wasn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'famous',\n",
              " ',',\n",
              " 'but',\n",
              " 'his',\n",
              " 'family',\n",
              " 'loved',\n",
              " 'his',\n",
              " 'book',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}